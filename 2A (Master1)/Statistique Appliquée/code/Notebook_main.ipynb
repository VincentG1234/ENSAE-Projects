{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21a231f6-6e35-4f82-a76a-83495d16091d",
   "metadata": {},
   "source": [
    "# Notebook principal de Vincent, projet Statapp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b9fa6d-66a7-43f8-9900-fae14cdd06e8",
   "metadata": {},
   "source": [
    "### chargement des libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "377065de-d7e3-42d2-8f1c-26d30a9a4d31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow==15.0\n",
      "  Downloading pyarrow-15.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.16.6 in /opt/mamba/lib/python3.12/site-packages (from pyarrow==15.0) (1.26.4)\n",
      "Downloading pyarrow-15.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (38.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 15.0.2\n",
      "    Uninstalling pyarrow-15.0.2:\n",
      "      Successfully uninstalled pyarrow-15.0.2\n",
      "Successfully installed pyarrow-15.0.0\n",
      "Collecting s3fs==2024.2.0\n",
      "  Downloading s3fs-2024.2.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /opt/mamba/lib/python3.12/site-packages (from s3fs==2024.2.0) (2.12.2)\n",
      "Collecting fsspec==2024.2.0 (from s3fs==2024.2.0)\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/mamba/lib/python3.12/site-packages (from s3fs==2024.2.0) (3.9.5)\n",
      "Requirement already satisfied: botocore<1.34.52,>=1.34.41 in /opt/mamba/lib/python3.12/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs==2024.2.0) (1.34.51)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /opt/mamba/lib/python3.12/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs==2024.2.0) (1.16.0)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /opt/mamba/lib/python3.12/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs==2024.2.0) (0.11.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/mamba/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2024.2.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/mamba/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2024.2.0) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/mamba/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2024.2.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/mamba/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2024.2.0) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/mamba/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2024.2.0) (1.9.4)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/mamba/lib/python3.12/site-packages (from botocore<1.34.52,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs==2024.2.0) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/mamba/lib/python3.12/site-packages (from botocore<1.34.52,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs==2024.2.0) (2.9.0)\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /opt/mamba/lib/python3.12/site-packages (from botocore<1.34.52,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs==2024.2.0) (1.26.18)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/mamba/lib/python3.12/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs==2024.2.0) (3.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/mamba/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.52,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs==2024.2.0) (1.16.0)\n",
      "Downloading s3fs-2024.2.0-py3-none-any.whl (28 kB)\n",
      "Downloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec, s3fs\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.3.1\n",
      "    Uninstalling fsspec-2024.3.1:\n",
      "      Successfully uninstalled fsspec-2024.3.1\n",
      "  Attempting uninstall: s3fs\n",
      "    Found existing installation: s3fs 2024.3.1\n",
      "    Uninstalling s3fs-2024.3.1:\n",
      "      Successfully uninstalled s3fs-2024.3.1\n",
      "Successfully installed fsspec-2024.2.0 s3fs-2024.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow==15.0\n",
    "!pip install s3fs==2024.2.0\n",
    "!pip install --quiet transformers\n",
    "!pip install --quiet datasets s3fs\n",
    "!pip install --quiet transformers[torch]\n",
    "!pip install --quiet accelerate -U\n",
    "!pip install --quiet matplotlib\n",
    "!pip install --quiet seaborn\n",
    "!pip install --quiet -U scikit-learn\n",
    "!pip install --quiet nltk\n",
    "!pip install --quiet wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f17ce4-7c81-437d-bd5b-4e7daaccd96b",
   "metadata": {},
   "source": [
    "Si vous n'utilisez pas le service du ssp cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fcf45db-29ca-4d36-a713-1ff790eed6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b42df5d-2da8-405d-849f-6b490160e224",
   "metadata": {},
   "source": [
    "### Importation des libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2df2133f-b30d-4346-8de1-20cb85d2d7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import GPT2Tokenizer, AutoTokenizer\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import GPT2LMHeadModel, AutoConfig\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import wandb\n",
    "from datasets import load_metric\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Modules maison\n",
    "import script_training\n",
    "import selection_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d63d72c-20db-4e49-b120-efef7fc602c9",
   "metadata": {},
   "source": [
    "# Pipeline d'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005ce231-0258-42cc-a540-11c0086f7b2b",
   "metadata": {},
   "source": [
    "## récupération dans mes fichiers du ssp cloud ensae des données et préprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e856aff-d1d0-4b97-a8df-9d6947e77511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import s3fs\n",
    "import json\n",
    "# Create filesystem object\n",
    "S3_ENDPOINT_URL = \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': S3_ENDPOINT_URL})\n",
    "LOGIN = \"vincentg\" # Le login Datalab, change d'une personne à l'autre.\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"       # plus besoin de toucher à ça ensuite\n",
    "os.environ[\"WAND_PROJECT\"] = \"exprmt\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d0108e4-6cfa-4625-85b7-b345673dc70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mamba/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "497db15081984938a275639b8e034e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f3484acd3d48ec9d1a49b53bff8c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e958663d969047db9cfc47d14d1f4c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0a449cac6a4b559d213a7d4c4dfb4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c086c3df70c4e6b9b0eaf7786791e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_287/2091263331.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  accuracy_metric = load_metric(\"accuracy\")\n",
      "/opt/mamba/lib/python3.12/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f37a0ca451149118e7933e9923f950e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "accuracy_metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4924ea2a-c19b-47fa-bf64-442bdb677d47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b2d9942a3e349ac9881d38caa6dfe24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/15.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95285de1a492410bb1d51521b12eff16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c8ea1e08d76471ebc09f40087561c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/257M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b328a312f24c9f92b151e663dcca38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/259M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bccf79c769a4b9681f037a320d17f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/34.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a924cacdac4576b105e5a7b9ea408f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/30.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d5f6c749d64ee1888c2cf571751955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "080e8f6627c643dba1bc808aa4368f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45b2050e5014137866629e5b6c13d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (204288 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Téléchargement des data de test: https://huggingface.co/datasets/cnn_dailymail?row=0\n",
    "\n",
    "data_test = load_dataset(\"cnn_dailymail\",  '1.0.0')\n",
    "data_test = data_test[\"test\"]\n",
    "encoded_test= tokenizer(\"\\n\\n\".join(data_test[\"article\"][:300]), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22bfcccc-3717-4ed2-b653-60cafb6b7e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/onyxia/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login() # rentrez le mdp: **************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fae7baf-5a61-47cb-ba4e-2ebbd7e66e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer, model, trainer, name = script_training.train_phase(train_val_size = [0.95, 0.05],n_embd=64, n_layer=4, n_head=4, batch_size = 16, epochs=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "42c10f3e-56cc-4a65-b3f5-cb92ab59edc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "script_training.main(file='Vendi_1200_100_238.json',\n",
    "               n_embd=384,\n",
    "               n_layer=6, \n",
    "               n_head=6, \n",
    "               train_val_size=[0.90, 0.1], \n",
    "               learning_rate=0.002, \n",
    "               batch_size=16, \n",
    "               epochs=2, \n",
    "               warmup_steps=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad23e696-a403-432c-bfbc-d0e5a8c606e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "script_training.main(file='Random_1200_233.json',\n",
    "          n_embd=384,\n",
    "          n_layer=6, \n",
    "          n_head=6, \n",
    "          train_val_size=[0.90, 0.1], \n",
    "          learning_rate=0.002, \n",
    "          batch_size=16, \n",
    "          epochs=2, \n",
    "          warmup_steps=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "af421e64-e60d-4965-9d26-d155277c9c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(captured_output.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328e5902-36b4-4b4f-a797-a408eff6bb0d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Fonction pour importer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7998e938-9c96-40b8-9a8b-89c206e655de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_directory(s3_path, local_path, s3=fs):\n",
    "    # Liste tous les objets (fichiers et sous-répertoires) dans le répertoire S3\n",
    "    objects = s3.ls(s3_path, detail=True)\n",
    "\n",
    "    # Crée le répertoire local s'il n'existe pas déjà\n",
    "    os.makedirs(local_path, exist_ok=True)\n",
    "\n",
    "    for obj in objects:\n",
    "        # Construit le chemin local pour chaque objet\n",
    "        local_obj_path = os.path.join(local_path, os.path.basename(obj['Key']))\n",
    "\n",
    "        if obj['StorageClass'] == 'DIRECTORY':\n",
    "            # Si l'objet est un sous-répertoire, appelle la fonction de manière récursive\n",
    "            download_directory(obj['Key'], local_obj_path)\n",
    "        else:\n",
    "            # Si l'objet est un fichier, télécharge-le localement\n",
    "            s3.get(obj['Key'], local_obj_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f43edd-b717-4d56-a054-f504ac55cb78",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## TEST sur GPT2 pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c888a685-01f2-4dfc-8c08-32d345bd3d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c48872-b15f-48bf-99b6-1668889b7753",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "script_training.main(file='tokenized_256_top100.json', model_=model, reprise=\"GPT2_pretrained_finetune_0.0006_0.5_top_100\", learning_rate=0.0006, train_val_size=[0.5, 0.02])\n",
    "print(captured_output.stdout) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4931fc80-de2b-4ec1-ab08-cf770b920a01",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Selection des corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d019221f-71db-48bf-a4ac-66836b191349",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fs.open(LOGIN+\"/StatApp/top_10_sur_10000.json\", 'r') as file:\n",
    "    data = Dataset.from_list(json.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7db170f7-fff1-4bb6-9d48-10aba72b7820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:52<00:00, 52.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min 15s, sys: 1.82 s, total: 21min 17s\n",
      "Wall time: 54.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = selection_corpus.select_least_similar3(data[\"text\"], \n",
    "                      size_selected = 1200,         \n",
    "                      num_subcorpus =  1,    \n",
    "                      tol = 10**(-2), \n",
    "                      affiche_entrop = False ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9216e943-e600-4f04-9bd2-d4b25b18f986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325a847d680642898a9e7e7af251c631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = X.map(lambda x :{'len': len(x[\"text\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a9728f72-d2ff-4ecc-b64e-14a72ccacd94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233.438234"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(X['len'])*2/1000**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c9b2b424-3ced-422f-abf8-78189b27dfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mamba/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Choisir un Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "\n",
    "def tokenize(element, tokenizer=None, context_length=256):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length= context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tokenized_to_cloud(data = None, context_length=256, user = \"vincentg\", file_name=None):    \n",
    "    tokenized_datasets = data.map(\n",
    "        lambda element: tokenize(element, tokenizer=tokenizer),\n",
    "        batched=True,remove_columns=data.column_names)\n",
    "    FILE_CHUNK_SHUFFLED_OUT_S3 = user + \"/StatApp/\" + file_name + \".json\"\n",
    "    with fs.open(FILE_CHUNK_SHUFFLED_OUT_S3, 'w') as f:\n",
    "    #json.dump(tokenized_datasets, f)\n",
    "    #tokenized_datasets.to_json(f)\n",
    "        json.dump(tokenized_datasets['input_ids'],f)\n",
    "    print(\"check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f2dd13a7-caeb-4f05-a010-58666ce9be21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204ced95e79f4d378be2892b47822d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check\n"
     ]
    }
   ],
   "source": [
    "tokenized_to_cloud(X, file_name=\"Random_1200_233\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8192de19-8b0a-487e-b3f3-e777be352950",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Tests divers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5d837163-ef8d-4927-b7a4-2ac1eae26664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "\n",
    "#tokenizer_test2 = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "#model_test2 = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "#tokenizer_test2.pad_token_id = tokenizer_test2.eos_token_id\n",
    "\n",
    "\n",
    "prompt3 = 'Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.'\n",
    "inputs = tokenizer.encode(prompt3, return_tensors=\"pt\").to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a5b60e00-eedb-41a1-8ac1-e91cf7713e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.\n",
      ": by The the a the and with the in and ( the || is\" was and of the\n",
      " to, at \" || ( a with the\n",
      " 16,E,, km) that. ||. || was on) and for is= of b —6 the) the  || align || with by in:,. for\n",
      "\n",
      " of-E the\"\n",
      "d\n",
      ". for aid's and ||E were| \"  The\n",
      " and —.. of\n",
      "1: Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry. to\n",
      "'s to on was for's- the || and that a ( the in || a of the and || to andg of to and|\n",
      " -.  ||| on — ||. the and || of right6- align for that andg'sd) the wasg to\n",
      " (\n",
      ". ||. to-'s of the= | fromd\n",
      " - \" on to by of with in the. in || tod || The of on || tog\n",
      "2: Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry. as'sg ('s, — km\"- by|), || by was, (. at align to the of the that.,=6 of that ||, as. that with'sE and a to9 in \", || a as's\n",
      " with km, was–\n",
      " from\n",
      " atg in\" a6 by| to to (), for to ( and6 of align– ||. is by theg. of and is to b-:9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=3,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c8b31e76-bc86-4fd9-ad16-6e86be9cc16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.\n",
      " the the\n",
      "\n",
      ", the || ||.. the. ||\n",
      "|\n",
      " || the, ||,. and the-\n",
      ".,\n",
      "- || a\n",
      " a the and ||  ||-. a,, and\n",
      " and.- the in the of the  the ( the a || (\n",
      " in. in || and- and,--, a. to\n",
      " to the to || in\n",
      " of\n",
      ". of || to. (, ( || — || align\n",
      "\n",
      "1: Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.\n",
      " the the\n",
      "\n",
      ", the || ||.. the. ||\n",
      "|\n",
      " || the, ||,. and the-\n",
      ".,\n",
      "- || a\n",
      " a the and ||  ||-. a,, and\n",
      " and.- the in the of the  the ( the a || (\n",
      " in. in || and- and,--, a. to\n",
      " to the to || in\n",
      " of\n",
      ". of || to. (, ( || —, in,\n",
      "2: Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.\n",
      " the the\n",
      "\n",
      ", the || ||.. the. ||\n",
      "|\n",
      " || the, ||,. and the-\n",
      ".,\n",
      "- || a\n",
      " a the and ||  ||-. a,, and\n",
      " and.- the in the of the  the ( the a || (\n",
      " in. in || and- and,--, a. to\n",
      " to the to || in\n",
      " of\n",
      ". of || to. (, ( || —, in-\n",
      "3: Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.\n",
      " the the\n",
      "\n",
      ", the || ||.. the. ||\n",
      "|\n",
      " || the, ||,. and the-\n",
      ".,\n",
      "- || a\n",
      " a the and ||  ||-. a,, and\n",
      " and.- the in the of the  the ( the a || (\n",
      " in. in || and- and,--, a. to\n",
      " to the to || in\n",
      " of\n",
      ". of || to. (, ( || —, in and\n",
      "4: Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry.\n",
      " the the\n",
      "\n",
      ", the || ||.. the. ||\n",
      "|\n",
      " || the, ||,. and the-\n",
      ".,\n",
      "- || a\n",
      " a the and ||  ||-. a,, and\n",
      " and.- the in the of the  the ( the a || (\n",
      " in. in || and- and,--, a. to\n",
      " to the to || in\n",
      " of\n",
      ". of || to. (, ( || — || align the\n"
     ]
    }
   ],
   "source": [
    "beam_outputs = model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens=100,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# now we have 3 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer_test2.decode(beam_output, skip_special_tokens=True)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
